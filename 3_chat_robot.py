import os
import uuid
import gradio as gr

# --- 1. ä¿®å¤ LangChain å¯¼å…¥ ---
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.output_parsers import StrOutputParser
from langchain_community.chat_message_histories import ChatMessageHistory # <--- å·²ä¿®æ”¹
from langchain_core.runnables.history import RunnableWithMessageHistory
from langchain_core.messages import SystemMessage

# ç¡®ä¿ API å¯†é’¥å·²è®¾ç½®
if "ARK_API_KEY" not in os.environ:
    raise ValueError("è¯·è®¾ç½®ç¯å¢ƒå˜é‡ ARK_API_KEY")

# åˆå§‹åŒ–æ¨¡å‹
chatARK = ChatOpenAI(
    model="deepseek-r1-250120",
    api_key=os.getenv("ARK_API_KEY"),
    base_url="https://ark.cn-beijing.volces.com/api/v3",
    temperature=0.7,
    streaming=True,
)

# åˆ›å»ºæç¤ºæ¨¡æ¿
# æ³¨æ„ï¼šä¸ºäº†é€‚é… Gradio çš„ 'messages' æ ¼å¼ï¼Œæˆ‘ä»¬ç¨å¾®è°ƒæ•´ä¸€ä¸‹ï¼Œç¡®ä¿ system prompt èƒ½è¢«æ­£ç¡®å¤„ç†
# å®é™…ä¸Š RunnableWithMessageHistory ä¼šè‡ªåŠ¨å¤„ç†ï¼Œä½†è¿™æ ·å†™æ›´æ¸…æ™°
prompt = ChatPromptTemplate.from_messages([
    MessagesPlaceholder(variable_name="history"),
    ("human", "{input}"),
])

# åˆ›å»ºè¾“å‡ºè§£æå™¨
output_parser = StrOutputParser()

# æ„å»ºæ ¸å¿ƒé“¾
# æˆ‘ä»¬åœ¨é“¾çš„æœ€å¼€å§‹åŠ å…¥ SystemMessageï¼Œè¿™æ ·å®ƒæ€»æ˜¯åœ¨å¯¹è¯çš„æœ€å‰é¢
chain_with_sys_prompt = (
    SystemMessage(content="ä½ å«Starryï¼Œæ˜¯ä¸€åä¹äºåŠ©äººçš„äººå·¥æ™ºèƒ½åŠ©æ‰‹ï¼Œè¯·ç”¨ä¸­æ–‡å›ç­”æ‰€æœ‰é—®é¢˜ã€‚")
    + prompt
)
core_chain = chain_with_sys_prompt | chatARK | output_parser

# è®¾ç½®è®°å¿†å­˜å‚¨
demo_ephemeral_chat_history_for_chain = {}

def get_session_history(session_id: str) -> ChatMessageHistory:
    if session_id not in demo_ephemeral_chat_history_for_chain:
        demo_ephemeral_chat_history_for_chain[session_id] = ChatMessageHistory()
    return demo_ephemeral_chat_history_for_chain[session_id]

# å°†æ ¸å¿ƒé“¾åŒ…è£…æˆå¸¦è®°å¿†çš„é“¾
conversational_chain = RunnableWithMessageHistory(
    core_chain,
    get_session_history,
    input_messages_key="input",
    history_messages_key="history",
)

# --- Gradio ç•Œé¢éƒ¨åˆ† ---

def predict(message, session_id: str):
    """Gradio çš„æ ¸å¿ƒé¢„æµ‹å‡½æ•°"""

    # æµå¼è°ƒç”¨é“¾
    stream = conversational_chain.stream(
        {"input": message},
        config={"configurable": {"session_id": session_id}}
    )

    # æµå¼è¿”å›
    is_first_chunk = True
    for chunk in stream:
        if is_first_chunk and not chunk.strip():
            continue
        if is_first_chunk:
            yield chunk.lstrip()
            is_first_chunk = False
        else:
            yield chunk

# --- 2. Gradio ç»„ä»¶åˆå§‹åŒ–å’Œå¸ƒå±€ ---
with gr.Blocks(
    theme=gr.themes.Soft(primary_hue="blue", secondary_hue="sky"),
    css="#chatbot { min-height: 600px; }"
) as demo:
    session_id_state = gr.State(str(uuid.uuid4()))

    gr.Markdown(
        """
        # ğŸ¤– Starry - ä½ çš„AIåŠ©æ‰‹
        ç”± LangChain å’Œç«å±±å¼•æ“å¤§æ¨¡å‹é©±åŠ¨ã€‚
        """
    )

    chatbot = gr.Chatbot(
        elem_id="chatbot",
        label="Starry",
        type="messages",
    )

    txt = gr.Textbox(
        show_label=False,
        placeholder="ä½ å¥½ï¼Œæœ‰ä»€ä¹ˆå¯ä»¥å¸®ä½ çš„å—ï¼Ÿ",
        container=False,
    )

    def add_text(history, text):
        # å¦‚æœç”¨æˆ·æ²¡è¾“å…¥å†…å®¹ï¼Œåˆ™ä¸åšä»»ä½•äº‹
        if not text:
            return history, gr.update(value="", interactive=True)
        history.append({"role": "user", "content": text})
        return history, gr.update(value="", interactive=False)

    def stream_message(history, session_id):
        user_message = history[-1]["content"]
        history[-1]["role"] = "user"
        history.append({"role": "assistant", "content": ""})

        stream = predict(user_message, history, session_id)
        for chunk in stream:
            history[-1]["content"] += chunk
            yield history

    # --- ä¿®å¤åçš„äº‹ä»¶é“¾ ---
    txt.submit(
        add_text,
        [chatbot, txt],
        [chatbot, txt]
    ).then(
        stream_message,
        [chatbot, session_id_state],
        chatbot
    ).then(
        lambda: gr.update(interactive=True),
        None,
        [txt]
    )

    demo.load(lambda: gr.update(interactive=True), None, [txt])

if __name__ == "__main__":
    demo.launch()
    
    # # ç¬¬ä¸€ç§å¸¦å…¥å†å²å¯¹è¯çš„æ–¹æ³•â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”
    # # åˆ›å»ºä¸€ä¸ªmessages_listå»å­˜å‚¨å†å²å¯¹è¯
    # messages_list=[]
    # print("ğŸ”¹ è¾“å…¥ exit ç»“æŸå¯¹è¯")
    # while True:
    #     user_query = input("ğŸ‘¤ ä½ ï¼š")
    #     if user_query.lower() in {"exit", "quit"}:
    #         break

    #     # 1) è¿½åŠ ç”¨æˆ·æ¶ˆæ¯
    #     messages_list.append(HumanMessage(content=user_query))

    #     # 2) è°ƒç”¨æ¨¡å‹
    #     assistant_reply = core_chain.invoke({"input":user_query,"history": messages_list})
    #     print("ğŸ¤– å°æ™ºï¼š", assistant_reply)

    #     # 3) è¿½åŠ  AI å›å¤
    #     messages_list.append(AIMessage(content=assistant_reply))

    #     # 4) ä»…ä¿ç•™æœ€è¿‘ 50 æ¡
    #     messages_list = messages_list[-50:]